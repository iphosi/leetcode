{"root":{"data":{"id":"d3yepgvv8eo0","created":1725545155194,"text":"PEFT"},"children":[{"data":{"id":"d3yepl6n7kw0","created":1725545164553,"text":"Prompt Tuning","layout_mind_offset":{"x":0,"y":-262.9999960809948}},"children":[{"data":{"id":"d3yeq3fen2w0","created":1725545204265,"text":"Hard Prompt","layout_right_offset":{"x":1.9999999701976776,"y":-38.99999941885466}},"children":[{"data":{"id":"d3yeu1wmw6g0","created":1725545514410,"text":"人工构建（Manual Template）"},"children":[]},{"data":{"id":"d3yeu2idry00","created":1725545515725,"text":"启发式法（Heuristic-based Template）"},"children":[]},{"data":{"id":"d3yeu2uzyhc0","created":1725545516487,"text":"生成（Generation）"},"children":[]}]},{"data":{"id":"d3yeq4jg5n40","created":1725545206686,"text":"Soft Prompt","layout_right_offset":{"x":0.9999999850988388,"y":17.999999731779127}},"children":[{"data":{"id":"d3yevindbrc0","created":1725545629219,"text":"词向量微调（Word Embedding）","layout_right_offset":{"x":-6.999999895691872,"y":-36.99999944865709}},"children":[]},{"data":{"id":"d3yevluvn600","created":1725545636203,"text":"伪标记（Pseudo Token）"},"children":[{"data":{"id":"d3yf3its55s0","created":1725546256520,"text":"P-Tuning","layout_right_offset":{"x":0,"y":-17.9999997317791}},"children":[{"data":{"id":"d3yf4a4aa880","created":1725546315928,"text":"将 Prompt 转换为可以学习的 Embedding 层，并用MLP+LSTM的方式来对 Prompt Embedding 进行一层处理"},"children":[{"data":{"id":"d3yfdj7mtc80","created":1725547040999,"text":"如果随机初始化virtual token，容易优化到局部最优值"},"children":[]}]},{"data":{"id":"d3yf4ebkgdc0","created":1725546325076,"text":"P-tuning 只限于 embedding 层"},"children":[]},{"data":{"id":"d3yfbpz864w0","created":1725546899000,"text":"virtual token 的位置不一定是前缀，插入的位置是可选的"},"children":[]}]},{"data":{"id":"d3yf3kuyjuo0","created":1725546260945,"text":"P-Tuning V2"},"children":[{"data":{"id":"d3yfigmxkhc0","created":1725547427215,"text":"每层都添加 virtual token 作为前缀"},"children":[]},{"data":{"id":"d3yfrjs6f1k0","created":1725548139340,"text":"不添加 MLP 结构"},"children":[]}]},{"data":{"id":"d3yf3l54n5c0","created":1725546261560,"text":"Prefix Tuning","layout_right_offset":{"x":0.9999999850988388,"y":32.99999950826171}},"children":[{"data":{"id":"d3yfigmxkhc0","created":1725547427215,"text":"每层都添加 virtual token 作为前缀"},"children":[]},{"data":{"id":"d3yfjfm4bfs0","created":1725547503353,"text":"为了防止直接更新 Prefix 的参数导致训练不稳定和性能下降的情况，在 Prefix 层前面加了 MLP 结构"},"children":[]}]}]}]}]},{"data":{"id":"d3yeprj6p1s0","created":1725545178372,"text":"Adapter Tuning","layout_mind_offset":{"x":484.999992772937,"y":486.99999350309395}},"children":[{"data":{"id":"d3yftl8j4k00","created":1725548299234,"text":"LoRA","layout_right_offset":{"x":462.9999950975181,"y":-19.999999046325684}},"children":[{"data":{"id":"d3yg5i8ulk00","created":1725549233092,"text":"超参数 α：α/r 缩放 LoRA 矩阵，相当于调整学习率","layout_right_offset":{"x":432.9999935477974,"y":1.9999999701976776}},"children":[]},{"data":{"id":"d3yg8fd9ms80","created":1725549461922,"text":"降维矩阵 A 采用高斯分布初始化，升维矩阵 B 初始化为全 0，以确保训练开始时旁路为 0 矩阵","layout_right_offset":{"x":677.9999898970129,"y":13.999999791383743}},"children":[{"data":{"id":"d3yg9qdyz740","created":1725549564273,"text":"都初始化为 0 会导致权重的对称性","layout_right_offset":{"x":821.9999882429839,"y":-23.999999314546585}},"children":[]},{"data":{"id":"d3ygagblqq80","created":1725549620726,"text":"都用高斯分布初始化会引入过大的偏置","layout_right_offset":{"x":837.9999877065422,"y":13.99999962747097}},"children":[]}]}]},{"data":{"id":"d3yftojs4tc0","created":1725548306444,"text":"AdaLoRA","layout_right_offset":{"x":461.9999946504832,"y":285.9999999701977}},"children":[{"data":{"id":"d3z2bya7w9k0","created":1725611802607,"text":"将 LoRA 矩阵的奇异值作为其重要性的指标，根据重要性调整矩阵的秩","layout_right_offset":{"x":591,"y":-31}},"children":[]}]},{"data":{"id":"d3z2hkus8340","created":1725612243560,"text":"DoRA","layout_right_offset":{"x":423,"y":296}},"children":[{"data":{"id":"d3z9vlzqkbk0","created":1725633090904,"text":"将参数矩阵分解为幅度向量 m 与方向矩阵 V 的乘积，对 V 添加 LoRA 矩阵，同时训练 m 和 LoRA 矩阵","layout_right_offset":{"x":742.9999889284373,"y":-24.999999627470856}},"children":[]}]},{"data":{"id":"d3z0b0j8twg0","created":1725606086922,"text":"LoRA+","layout_right_offset":{"x":457,"y":-83}},"children":[{"data":{"id":"d3z0bevy12g0","created":1725606118165,"text":"为矩阵 A 和 B 引入不同的学习率","layout_right_offset":{"x":360,"y":-21}},"children":[]}]},{"data":{"id":"d3z0jdl87bc0","created":1725606742253,"text":"VeRA","layout_right_offset":{"x":456,"y":-60}},"children":[{"data":{"id":"d3z0k6kvi880","created":1725606805359,"text":"用共享的随机权值初始化矩阵并冻结，引入两个可训练向量","layout_right_offset":{"x":591,"y":-42}},"children":[]}]},{"data":{"id":"d3z0jl5xjeo0","created":1725606758743,"text":"LoRA-FA","layout_right_offset":{"x":518,"y":-8}},"children":[{"data":{"id":"d3z0lk9sfi80","created":1725606913528,"text":"冻结矩阵 A","layout_right_offset":{"x":255,"y":-31}},"children":[]}]},{"data":{"id":"d3yftost86o0","created":1725548306990,"text":"QLoRA","layout_right_offset":{"x":407.99999496340763,"y":182.9999997317791}},"children":[{"data":{"id":"d3z2xjgobwg0","created":1725613494357,"text":"首先将LLM进行4位量化，从而显著减少模型的内存占用。接着，使用低阶适配器（LoRA）方法对量化的LLM进行微调。","layout_right_offset":{"x":830,"y":-31}},"children":[]},{"data":{"id":"d3z2yqbury80","created":1725613587667,"text":"引入了 4 位量化、4 位 NormalFloat 数据类型、双量化和分页优化器","layout_right_offset":{"x":572,"y":19}},"children":[]}]}]}]},"template":"default","theme":"fresh-blue","version":"1.4.33"}